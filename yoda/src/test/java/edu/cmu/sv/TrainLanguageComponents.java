package edu.cmu.sv;

import com.google.common.collect.*;
import edu.cmu.sv.natural_language_generation.CorpusGeneration;
import edu.cmu.sv.ontology.Ontology;
import edu.cmu.sv.ontology.role.has_quality_subroles.HasQualityRole;
import edu.cmu.sv.semantics.SemanticsModel;
import edu.cmu.sv.spoken_language_understanding.nested_chunking_understander.Chunker;
import edu.cmu.sv.spoken_language_understanding.nested_chunking_understander.ChunkingProblem;
import edu.cmu.sv.spoken_language_understanding.nested_chunking_understander.MultiClassifier;
import edu.cmu.sv.spoken_language_understanding.nested_chunking_understander.NodeMultiClassificationProblem;
import edu.cmu.sv.utils.StringDistribution;
import org.apache.commons.lang3.tuple.ImmutablePair;
import org.apache.commons.lang3.tuple.Pair;
import org.json.simple.JSONObject;
import org.junit.Test;

import java.io.*;
import java.util.*;
import java.util.stream.Collectors;

/**
 * Created by David Cohen on 10/29/14.
 *
 * Generate an artificial corpus and use it to train language components (SLU / LM)
 */
public class TrainLanguageComponents {
    public static int inVocabCutoff = 10;

    @Test
    public void Test() throws FileNotFoundException, UnsupportedEncodingException {
        Multiset<String> featureCounter = HashMultiset.create();
        Multiset<String> vocabularyCounter = HashMultiset.create();
        LinkedList<String> chunkingOutputLabels = new LinkedList<>(Arrays.asList(Chunker.NO_LABEL));
        LinkedList<String> chunkingContextFeatures = new LinkedList<>();
        HashMap<String, LinkedList<String>> classificationVariablesAndRanges = new HashMap<>();

        Set<ChunkingProblem> chunkingProblems = new HashSet<>();
        Set<NodeMultiClassificationProblem> multiClassificationProblems = new HashSet<>();
        System.out.println("generating corpus");
        List<Map.Entry<String, SemanticsModel>> corpus = new LinkedList<>();
//        corpus = CorpusGeneration.generateCorpus2();

        Map<String, SemanticsModel> oldCorpus = CorpusGeneration.generateCorpus();
        oldCorpus.keySet().forEach(x -> corpus.add(new AbstractMap.SimpleEntry<String, SemanticsModel>(x, oldCorpus.get(x))));

        System.out.println("corpus size:" + corpus.size());

        System.out.println("collecting features and training samples");

//        for (String utterance : corpus.keySet()){
//            System.out.println(utterance);
//            System.out.println(corpus.get(utterance).getInternalRepresentation().toJSONString());
//            System.out.println("---");
//        }
//        System.exit(0);


        for (Map.Entry<String, SemanticsModel> utteranceMeaningPair : corpus) {
            String utterance = utteranceMeaningPair.getKey();
            SemanticsModel utteranceMeaning = utteranceMeaningPair.getValue();

            for (String slotPath : Iterables.concat(utteranceMeaning.getAllInternalNodePaths(), Arrays.asList(""))) {
                JSONObject groundTruthContent = (JSONObject) utteranceMeaning.newGetSlotPathFiller(slotPath);
                // don't train a classifier within an adjective node, since they won't be generated by chunking
                if (groundTruthContent.containsKey("class") &&
                        Ontology.adjectiveClasses.contains(
                                Ontology.thingNameMap.get(groundTruthContent.get("class"))))
                    continue;

                //// build classification problem at this node

                NodeMultiClassificationProblem multiClassificationProblem =
                        new NodeMultiClassificationProblem(utterance, utteranceMeaning.getInternalRepresentation(), slotPath);
                featureCounter.addAll(MultiClassifier.extractFeatures(multiClassificationProblem));


                Map<String, Object> classificationResults = new HashMap<>();
                for (Object key : groundTruthContent.keySet()) {
                    if (key.equals("class") || key.equals("dialogAct"))
                        classificationResults.put((String) key, groundTruthContent.get(key));
                    else if (Ontology.thingNameMap.containsKey(key) &&
                            HasQualityRole.class.isAssignableFrom(Ontology.thingNameMap.get(key))) {
                        classificationResults.put((String) key, ((JSONObject) groundTruthContent.get(key)).get("class"));
                    }

                }

                // collect the full set of classification results and keys that appear in the data
                for (String key : classificationResults.keySet()) {
                    if (!classificationVariablesAndRanges.containsKey(key))
                        classificationVariablesAndRanges.put(key, new LinkedList<>(Arrays.asList(MultiClassifier.NOT_CLASSIFIED)));
                    if (!classificationVariablesAndRanges.get(key).contains(classificationResults.get(key)))
                        classificationVariablesAndRanges.get(key).add((String) classificationResults.get(key));
                }

                multiClassificationProblem.outputDistribution = new StringDistribution();
                multiClassificationProblem.outputDistribution.put("ground_truth", 1.0);
                multiClassificationProblem.outputRolesAndFillers = new HashMap<>();
                multiClassificationProblem.outputRolesAndFillers.put("ground_truth", classificationResults);
                multiClassificationProblems.add(multiClassificationProblem);


                //// build chunking problem at this node
                if (slotPath.equals("") ||
                        (groundTruthContent.containsKey("chunk-start") && groundTruthContent.containsKey("chunk-end"))) {

                    Pair<Integer, Integer> chunkingIndices = SemanticsModel.getChunkingIndices(groundTruthContent);
                    ChunkingProblem chunkingProblem = new ChunkingProblem(
                            utterance,
                            utteranceMeaning.getInternalRepresentation(),
                            slotPath,
                            chunkingIndices);

                    List<List<String>> sequenceFeatures = Chunker.extractSequenceFeatures(chunkingProblem);
                    sequenceFeatures.forEach(vocabularyCounter::addAll);

                    Chunker.extractContextFeatures(chunkingProblem).stream().
                            filter(contextFeature -> !chunkingContextFeatures.contains(contextFeature)).
                            forEach(chunkingContextFeatures::add);

                    Set<ChunkingProblem> childChunkingProblems = new HashSet<>();

                    // extract child chunking problems by recursively searching for descendants with chunking indices
                    Set<Pair<String, JSONObject>> activeChildren = new HashSet<>();

                    for (Object key : groundTruthContent.keySet()) {
                        if (groundTruthContent.get(key) instanceof JSONObject) {
                            activeChildren.add(new ImmutablePair<>(
                                    (String) key,
                                    (JSONObject) groundTruthContent.get(key)));
                        }
                    }

                    while (!activeChildren.isEmpty()) {
                        Iterator<Pair<String, JSONObject>> it = activeChildren.iterator();
                        Pair<String, JSONObject> child = it.next();
                        it.remove();
                        if (child.getValue().containsKey("chunk-start")) {
                            if (!chunkingOutputLabels.contains(child.getLeft() + "-B")) {
                                chunkingOutputLabels.add(child.getLeft() + "-B");
                                chunkingOutputLabels.add(child.getLeft() + "-I");
                            }

                            Pair<Integer, Integer> childChunkingIndices = SemanticsModel.getChunkingIndices(child.getRight());
                            String childSlotPath = child.getLeft();
                            if (!slotPath.equals(""))
                                childSlotPath = slotPath + "." + childSlotPath;

                            ChunkingProblem childChunkingProblem = new ChunkingProblem(
                                    utterance,
                                    utteranceMeaning.getInternalRepresentation(),
                                    childSlotPath,
                                    childChunkingIndices);
                            childChunkingProblems.add(childChunkingProblem);
                        } else {
                            for (Object key : child.getRight().keySet()) {
                                if (child.getRight().get(key) instanceof JSONObject) {
                                    activeChildren.add(new ImmutablePair<>(
                                            child.getLeft() + "." + key,
                                            (JSONObject) child.getRight().get(key)));
                                }
                            }
                        }
                    }

                    chunkingProblem.outputDistribution = new StringDistribution();
                    chunkingProblem.outputDistribution.put("ground_truth", 1.0);
                    chunkingProblem.outputChildChunkingProblems = new HashMap<>();
                    chunkingProblem.outputChildChunkingProblems.put("ground_truth", childChunkingProblems);
                    chunkingProblems.add(chunkingProblem);
                }
            }
        }

//        System.out.println("feature counter's size:" + featureCounter.size());
//        for (String feature : featureCounter.elementSet()){
//            System.out.println("feature:    "+feature+", count:    "+featureCounter.count(feature));
//        }

        //// write out chunker preferences and training file
        {
            // select chunking features from those that were seen
            LinkedList<String> retainedVocabulary = new LinkedList(vocabularyCounter.elementSet().stream().
                    filter(x -> vocabularyCounter.count(x) > inVocabCutoff).collect(Collectors.toList()));
            retainedVocabulary.add(0, Chunker.UNK);

            // retain all context features and output labels

            // write out chunking preferences
            try {
                ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(Chunker.serializedChunkerPreferencesFile));
                out.writeObject(new ArrayList<>(Arrays.asList(
                        chunkingContextFeatures,
                        retainedVocabulary,
                        chunkingOutputLabels)));
                out.close();
            } catch (IOException e) {
                e.printStackTrace();
            }

            // write out training file
            Chunker.loadPreferences();
            System.out.println("writing chunker training file...");
            PrintWriter writer = new PrintWriter(Chunker.chunkerTrainingFile, "UTF-8");
            for (ChunkingProblem problem : chunkingProblems) {
                writer.write(Chunker.packTrainingSample(problem) + "\n");
            }
            writer.close();
        }

        // train model
        System.out.println("training theano chunker model ...");
        System.out.println("labels:" + Chunker.outputLabelKey);
        Chunker.trainTheanoModel();
        System.out.println("done training chunker model.");


        // remove from the classifier any roles which are handled exclusively by the chunker
        Set<String> rolesHandledExclusivelyByChunker = new HashSet<>();
        for (String outputLabel : chunkingOutputLabels){
            outputLabel = outputLabel.replace("-B","").replace("-I","");
            rolesHandledExclusivelyByChunker.addAll(Arrays.asList(outputLabel.split("\\.")));
        }
        System.out.println("all possible classification variables:\n" + classificationVariablesAndRanges.keySet());
        rolesHandledExclusivelyByChunker.forEach(classificationVariablesAndRanges::remove);
        System.out.println("classification variables not already handled by chunker:\n" + classificationVariablesAndRanges.keySet());

        //// write out multi-classifier preferences and training file
        {
            // select classification features from those that were generated
            List<String> retainedFeatures = featureCounter.elementSet().stream().
                    filter(x -> featureCounter.count(x) > 1).collect(Collectors.toList());
            HashMap<String, Integer> featurePositionMap = new HashMap<>();
            for (int i = 0; i < retainedFeatures.size(); i++) {
                featurePositionMap.put(retainedFeatures.get(i), i);
            }

            // write out classification preferences
            try {
                ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(MultiClassifier.serializedClassifierPreferencesFile));
                out.writeObject(new ArrayList<>(Arrays.asList(featurePositionMap, (Serializable) classificationVariablesAndRanges)));
                out.close();
            } catch (IOException e) {
                e.printStackTrace();
            }

            // write out multi-classifier training file
            MultiClassifier.loadPreferences();
            System.out.println("writing multi-classifier training file...");
            PrintWriter writer = new PrintWriter(MultiClassifier.classifierTrainingFile, "UTF-8");
            for (Object classificationProblem : multiClassificationProblems) {
                writer.write(MultiClassifier.packTrainingSample((NodeMultiClassificationProblem) classificationProblem) + "\n");
            }
            writer.close();
        }

        // train model
        System.out.println("training theano classifier model ...");
        MultiClassifier.trainTheanoModel();
        System.out.println("done training classifier model.");
    }
}
